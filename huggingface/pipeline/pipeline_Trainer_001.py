from datasets import load_dataset

# å‡†å¤‡æ•°æ®é›†
# åŠ è½½Yelpè¯„è®ºæ•°æ®é›†ï¼š
dataset = load_dataset("yelp_review_full")
print(dataset["train"][100])

from transformers import AutoTokenizer
# tokenizeræ¥å¤„ç†æ–‡æœ¬ï¼ŒåŒ…æ‹¬å¡«å……å’Œæˆªæ–­æ“ä½œä»¥å¤„ç†å¯å˜çš„åºåˆ—é•¿åº¦
tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-cased")
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)
# Datasets çš„ map æ–¹æ³•ï¼Œå°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†
tokenized_datasets = dataset.map(tokenize_function, batched=True)
# tokenized_datasets = small_train_dataset.map(tokenize_function, batched=True)

# å®Œæ•´æ•°æ®é›†æå–ä¸€ä¸ªè¾ƒå°å­é›†æ¥è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘è®­ç»ƒæ‰€éœ€çš„æ—¶é—´ï¼š
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(100))
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(100))

# å¼€å§‹è®­ç»ƒ
'''
 Transformers æä¾›äº†ä¸€ä¸ªä¸“ä¸ºè®­ç»ƒ Transformers æ¨¡å‹è€Œä¼˜åŒ–çš„ Trainer ç±»ï¼Œä½¿æ‚¨æ— éœ€æ‰‹åŠ¨ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ­¥éª¤è€Œæ›´è½»æ¾åœ°å¼€å§‹è®­ç»ƒæ¨¡å‹
 1.æŒ‡å®šæ¨¡å‹
 2.æŒ‡å®šè®­ç»ƒè¶…å‚æ•°
 3.æŒ‡å®šè¯„ä¼°å‡½æ•°
 4.åˆ›å»ºè®­ç»ƒå™¨
 5.å¾®è°ƒæ¨¡å‹
'''
from transformers import AutoModelForSequenceClassification
# 1.æŒ‡å®šæ¨¡å‹
# åŠ è½½æ‚¨çš„æ¨¡å‹å¹¶æŒ‡å®šæœŸæœ›çš„æ ‡ç­¾æ•°é‡,å·²çŸ¥æ•°æ®é›†æœ‰äº”ä¸ªæ ‡ç­¾
model = AutoModelForSequenceClassification.from_pretrained("google-bert/bert-base-cased", num_labels=5)

# 2.æŒ‡å®šè®­ç»ƒè¶…å‚æ•°
# åˆ›å»ºä¸€ä¸ª TrainingArguments ç±»ï¼Œå…¶ä¸­åŒ…å«æ‚¨å¯ä»¥è°ƒæ•´çš„æ‰€æœ‰è¶…å‚æ•°ä»¥åŠç”¨äºæ¿€æ´»ä¸åŒè®­ç»ƒé€‰é¡¹çš„æ ‡å¿—ã€‚å¯¹äºæœ¬æ•™ç¨‹ï¼Œæ‚¨å¯ä»¥ä»é»˜è®¤çš„è®­ç»ƒè¶…å‚æ•°å¼€å§‹ï¼Œä½†éšæ—¶å¯ä»¥å°è¯•ä¸åŒçš„è®¾ç½®ä»¥æ‰¾åˆ°æœ€ä½³è®¾ç½®ã€‚
from transformers import TrainingArguments
training_args = TrainingArguments(output_dir="test_trainer")

# 3.æŒ‡å®šè¯„ä¼°å‡½æ•°
# Trainer åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šè‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚æ‚¨éœ€è¦å‘ Trainer ä¼ é€’ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å’Œå±•ç¤ºæŒ‡æ ‡ã€‚ğŸ¤— Evaluate åº“æä¾›äº†ä¸€ä¸ªç®€å•çš„ accuracy å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ evaluate.load å‡½æ•°åŠ è½½å®ƒ
import numpy as np
import evaluate
metric = evaluate.load("accuracy")

# åœ¨ metric ä¸Šè°ƒç”¨ compute æ¥è®¡ç®—æ‚¨çš„é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨å°†é¢„æµ‹ä¼ é€’ç»™ compute ä¹‹å‰ï¼Œæ‚¨éœ€è¦å°†é¢„æµ‹è½¬æ¢ä¸ºlogitsï¼ˆè¯·è®°ä½ï¼Œæ‰€æœ‰ ğŸ¤— Transformers æ¨¡å‹éƒ½è¿”å›å¯¹logitsï¼‰
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

# åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç›‘è§†è¯„ä¼°æŒ‡æ ‡ï¼Œè¯·åœ¨æ‚¨çš„è®­ç»ƒå‚æ•°ä¸­æŒ‡å®š eval_strategy å‚æ•°ï¼Œä»¥åœ¨æ¯ä¸ªepochç»“æŸæ—¶å±•ç¤ºè¯„ä¼°æŒ‡æ ‡ï¼š
from transformers import TrainingArguments, Trainer
training_args = TrainingArguments(output_dir="test_trainer", eval_strategy="epoch")

# 4.åˆ›å»ºè®­ç»ƒå™¨
# åˆ›å»ºä¸€ä¸ªåŒ…å«æ‚¨çš„æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä»¥åŠè¯„ä¼°å‡½æ•°çš„ Trainer å¯¹è±¡
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics,
)
# 5.å¾®è°ƒæ¨¡å‹
# è°ƒç”¨train()ä»¥å¾®è°ƒæ¨¡å‹
trainer.train()